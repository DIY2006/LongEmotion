import os
import json
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.optim import AdamW
from tqdm import tqdm

# ==========================
# 基本配置
# ==========================
class Config:
    model_name = r"D:\Projects\LongEmotion_Analysis\local_phi15"  # 模型路径（本地或HF）
    data_path = "./data/ec_train.jsonl"                           # 训练数据
    save_dir = "./trained_model"                                  # 模型保存目录
    epochs = 1                                                    # 只跑一轮（可改）
    batch_size = 1                                                # 显存小设为1
    lr = 3e-5
    max_len = 128
    device = "cuda" if torch.cuda.is_available() else "cpu"

# ==========================
# 数据集定义
# ==========================
class EmotionDataset(Dataset):
    def __init__(self, path, tokenizer, max_len):
        with open(path, "r", encoding="utf-8") as f:
            self.data = [json.loads(line) for line in f]
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = self.data[idx].get("text", "")
        tokens = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_len,
            return_tensors="pt"
        )
        return {k: v.squeeze(0) for k, v in tokens.items()}

# ==========================
# 训练主函数
# ==========================
def train():
    cfg = Config()
    os.makedirs(cfg.save_dir, exist_ok=True)
    log_path = os.path.join(cfg.save_dir, "train_log.txt")

    print("🧩 加载模型与分词器...")
    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)
    model = AutoModelForCausalLM.from_pretrained(cfg.model_name)
    model.to(cfg.device)

    dataset = EmotionDataset(cfg.data_path, tokenizer, cfg.max_len)
    loader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=True)

    optimizer = AdamW(model.parameters(), lr=cfg.lr)

    print(f"🚀 开始训练，共 {cfg.epochs} 轮...")
    model.train()

    for epoch in range(cfg.epochs):
        total_loss = 0
        progress = tqdm(loader, desc=f"Epoch {epoch+1}/{cfg.epochs}")

        for batch in progress:
            batch = {k: v.to(cfg.device) for k, v in batch.items()}
            outputs = model(**batch, labels=batch["input_ids"])
            loss = outputs.loss

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            progress.set_postfix(loss=loss.item())

        avg_loss = total_loss / len(loader)
        print(f"✅ Epoch {epoch+1} 完成 | 平均损失: {avg_loss:.4f}")

        # 保存模型
        torch.save(model.state_dict(), os.path.join(cfg.save_dir, f"model_epoch{epoch+1}.pt"))
        # 保存日志
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(f"Epoch {epoch+1}: loss={avg_loss:.4f}\n")

    print("🎉 训练结束！日志与模型已保存。")

# ==========================
# 程序入口
# ==========================
if __name__ == "__main__":
    torch.cuda.empty_cache()
    train()
