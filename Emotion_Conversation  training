import os
import json
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.optim import AdamW
from tqdm import tqdm

# ==========================
# åŸºæœ¬é…ç½®
# ==========================
class Config:
    model_name = r"D:\Projects\LongEmotion_Analysis\local_phi15"  # æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°æˆ–HFï¼‰
    data_path = "./data/ec_train.jsonl"                           # è®­ç»ƒæ•°æ®
    save_dir = "./trained_model"                                  # æ¨¡å‹ä¿å­˜ç›®å½•
    epochs = 1                                                    # åªè·‘ä¸€è½®ï¼ˆå¯æ”¹ï¼‰
    batch_size = 1                                                # æ˜¾å­˜å°è®¾ä¸º1
    lr = 3e-5
    max_len = 128
    device = "cuda" if torch.cuda.is_available() else "cpu"

# ==========================
# æ•°æ®é›†å®šä¹‰
# ==========================
class EmotionDataset(Dataset):
    def __init__(self, path, tokenizer, max_len):
        with open(path, "r", encoding="utf-8") as f:
            self.data = [json.loads(line) for line in f]
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = self.data[idx].get("text", "")
        tokens = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_len,
            return_tensors="pt"
        )
        return {k: v.squeeze(0) for k, v in tokens.items()}

# ==========================
# è®­ç»ƒä¸»å‡½æ•°
# ==========================
def train():
    cfg = Config()
    os.makedirs(cfg.save_dir, exist_ok=True)
    log_path = os.path.join(cfg.save_dir, "train_log.txt")

    print("ğŸ§© åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)
    model = AutoModelForCausalLM.from_pretrained(cfg.model_name)
    model.to(cfg.device)

    dataset = EmotionDataset(cfg.data_path, tokenizer, cfg.max_len)
    loader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=True)

    optimizer = AdamW(model.parameters(), lr=cfg.lr)

    print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œå…± {cfg.epochs} è½®...")
    model.train()

    for epoch in range(cfg.epochs):
        total_loss = 0
        progress = tqdm(loader, desc=f"Epoch {epoch+1}/{cfg.epochs}")

        for batch in progress:
            batch = {k: v.to(cfg.device) for k, v in batch.items()}
            outputs = model(**batch, labels=batch["input_ids"])
            loss = outputs.loss

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            progress.set_postfix(loss=loss.item())

        avg_loss = total_loss / len(loader)
        print(f"âœ… Epoch {epoch+1} å®Œæˆ | å¹³å‡æŸå¤±: {avg_loss:.4f}")

        # ä¿å­˜æ¨¡å‹
        torch.save(model.state_dict(), os.path.join(cfg.save_dir, f"model_epoch{epoch+1}.pt"))
        # ä¿å­˜æ—¥å¿—
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(f"Epoch {epoch+1}: loss={avg_loss:.4f}\n")

    print("ğŸ‰ è®­ç»ƒç»“æŸï¼æ—¥å¿—ä¸æ¨¡å‹å·²ä¿å­˜ã€‚")

# ==========================
# ç¨‹åºå…¥å£
# ==========================
if __name__ == "__main__":
    torch.cuda.empty_cache()
    train()
